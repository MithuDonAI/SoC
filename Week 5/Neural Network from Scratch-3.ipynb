{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c672a8a2",
   "metadata": {},
   "source": [
    "### 13. What Is Backpropagation?\n",
    "\n",
    "Once we compute the **loss**, we need to understand how to change each **parameter** to reduce it. This is the goal of **backpropagation**.\n",
    "\n",
    "**Backpropagation** is just a fancy name for using the **chain rule** from calculus to compute **gradients** of the loss with respect to weights and biases.\n",
    "\n",
    "Let’s say the total loss is $\\mathcal{L}$.\n",
    "\n",
    "We want to compute:\n",
    "\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial W}$: How changing the weights affects loss\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial b}$: How changing biases affects loss\n",
    "\n",
    "This allows us to **update** these parameters to make the network better.\n",
    "\n",
    "We go **layer-by-layer from last to first**, applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(2)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$: network's prediction\n",
    "- $z^{(2)}$: raw score before sigmoid\n",
    "- $W^{(2)}$: weig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414645a",
   "metadata": {},
   "source": [
    "### 14. Gradients at Each Layer\n",
    "\n",
    "Let’s go step-by-step:\n",
    "\n",
    "1. **Loss Layer (Binary Crossentropy)**\n",
    "   - We already derived:\n",
    "     $$\n",
    "     \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}\n",
    "     $$\n",
    "\n",
    "2. **Sigmoid Layer**\n",
    "   - Sigmoid's derivative:\n",
    "     $$\n",
    "     \\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})\n",
    "     $$\n",
    "\n",
    "3. **Dense Layer**\n",
    "   - For weights:\n",
    "     $$\n",
    "     \\frac{\\partial z}{\\partial W} = a^{(1)} \\quad (\\text{input from previous layer})\n",
    "     $$\n",
    "   - For biases:\n",
    "     $$\n",
    "     \\frac{\\partial z}{\\partial b} = 1\n",
    "     $$\n",
    "\n",
    "All these are computed in code as:\n",
    "- `dvalues`: gradient coming from next layer\n",
    "- `dweights = inputs.T @ dvalues`\n",
    "- `dbiases = np.sum(dvalues)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_function.backward(activation2.output, y)\n",
    "activation2.backward(loss_function.dinputs)\n",
    "dense2.backward(activation2.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d045559",
   "metadata": {},
   "source": [
    "### 15. Why Are Gradients Important?\n",
    "\n",
    "Gradients tell us:\n",
    "\n",
    "- Which direction (increase or decrease) to change weights\n",
    "- By how much\n",
    "\n",
    "If $\\frac{\\partial \\mathcal{L}}{\\partial w}$ is:\n",
    "- Positive → reduce the weight\n",
    "- Negative → increase the weight\n",
    "\n",
    "This is why we use:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "This update step is done by an **optimizer**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddec33",
   "metadata": {},
   "source": [
    "### 16. Optimizer: Adam\n",
    "\n",
    "**Adam** stands for:\n",
    "- **A**daptive\n",
    "- **M**oment\n",
    "- **E**stimation\n",
    "\n",
    "It combines:\n",
    "- **Momentum**: smooths gradient using moving average\n",
    "- **RMSprop**: adapts learning rate for each parameter\n",
    "\n",
    "Adam keeps:\n",
    "- **m**: moving average of gradients (1st moment)\n",
    "- **v**: moving average of squared gradients (2nd moment)\n",
    "\n",
    "Then performs update:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$: parameter (weight or bias)\n",
    "- $\\eta$: learning rate\n",
    "- $\\epsilon$: small constant to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09634bb",
   "metadata": {},
   "source": [
    "### 17. Training Loop (Putting Everything Together)\n",
    "\n",
    "Each epoch:\n",
    "1. Forward pass through all layers\n",
    "2. Compute loss and accuracy\n",
    "3. Backward pass through all layers\n",
    "4. Optimizer updates weights\n",
    "\n",
    "This is repeated for many epochs (iterations over dataset), slowly improving the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb745fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2470ce1",
   "metadata": {},
   "source": [
    "### 18. Regularization\n",
    "\n",
    "Regularization discourages weights from growing too large.\n",
    "\n",
    "We add penalties to the loss function:\n",
    "\n",
    "#### L2 (Ridge) Regularization:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{reg} = \\lambda \\sum w^2\n",
    "$$\n",
    "\n",
    "#### L1 (Lasso) Regularization:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{reg} = \\lambda \\sum |w|\n",
    "$$\n",
    "\n",
    "This helps the model **generalize better** and not memorize training data.\n",
    "\n",
    "The total loss becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{total} = \\mathcal{L}_{data} + \\mathcal{L}_{reg}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e92e06",
   "metadata": {},
   "source": [
    "### 19. Validation Step\n",
    "\n",
    "We evaluate model performance on **unseen test data** to ensure it generalizes.\n",
    "\n",
    "Steps:\n",
    "1. Forward pass on test data\n",
    "2. Compute test loss and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
