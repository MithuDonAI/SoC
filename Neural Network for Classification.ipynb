{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7aed2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d691926",
   "metadata": {},
   "source": [
    "Fully Connected Layer (Layer_Dense)\n",
    "This class implements a dense layer, also known as a fully connected layer.\n",
    "\n",
    " __init__: Layer Initialization\n",
    "Initializes:\n",
    "\n",
    "Weights: small random numbers scaled by 0.01 (to break symmetry).\n",
    "\n",
    "Biases: initialized to zeros.\n",
    "\n",
    "Stores L1 and L2 regularization strength for both weights and biases.\n",
    "\n",
    " forward(): Forward Pass\n",
    "Computes the output using:\n",
    "\n",
    "output\n",
    "=\n",
    "ùëã\n",
    "‚ãÖ\n",
    "ùëä\n",
    "+\n",
    "ùëè\n",
    "output=X‚ãÖW+b\n",
    "Saves inputs for use in backpropagation.\n",
    "\n",
    " backward(): Backward Pass\n",
    "Calculates gradients:\n",
    "\n",
    "dweights, dbiases: gradients of loss w.r.t. weights/biases.\n",
    "\n",
    "Adds L1 and L2 regularization terms if applicable.\n",
    "\n",
    "dinputs: gradient to pass to the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c6da61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        # Store regularization strengths\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs  # Save input values for use in backward pass\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on weights and biases\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Regularization - L1 and L2 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        # Regularization - L1 and L2 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on inputs (for previous layer)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f075b9",
   "metadata": {},
   "source": [
    "\n",
    "Dropout Regularization (Layer_Dropout)\n",
    "Dropout is a regularization technique used to prevent overfitting.\n",
    "\n",
    "During training:\n",
    "\n",
    "A random subset of neurons is \"dropped\" (set to 0).\n",
    "\n",
    "This forces the network to not depend too heavily on any one neuron.\n",
    "\n",
    "At inference time, dropout is not used.\n",
    "\n",
    " forward():\n",
    "Creates a binary mask (0s and 1s) using a binomial distribution.\n",
    "\n",
    "Applies this mask to the input, scaling the remaining values to maintain output magnitude.\n",
    "\n",
    "backward():\n",
    "Applies the same binary mask to the gradient coming from the next layer to block the flow of gradients through dropped neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d10b9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                                              size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268acc2",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    " SECTION 4: ReLU Activation Function\n",
    " Markdown Cell (Explanation):\n",
    " Activation Function: ReLU (Rectified Linear Unit)\n",
    "ReLU is a non-linear activation function used in hidden layers.\n",
    "It allows the network to learn complex relationships while avoiding vanishing gradients.\n",
    "\n",
    " forward():\n",
    "Applies the function:\n",
    "\n",
    "ReLU\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "max\n",
    "‚Å°\n",
    "(\n",
    "0\n",
    ",\n",
    "ùë•\n",
    ")\n",
    "ReLU(x)=max(0,x)\n",
    "Any negative values are set to 0; positive values remain unchanged.\n",
    "\n",
    "backward():\n",
    "During backpropagation, gradients are allowed to pass through only where the input was positive.\n",
    "\n",
    "If the input was ‚â§ 0, the gradient is set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76ee936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb1418",
   "metadata": {},
   "source": [
    " Activation Function: Softmax\n",
    "Softmax is used in the output layer for multi-class classification.\n",
    "It converts raw scores (logits) into normalized probabilities that sum to 1.\n",
    "\n",
    "forward():\n",
    "Computes exponentials of inputs (after stabilizing by subtracting max).\n",
    "\n",
    "Normalizes them so that for each sample, the output vector sums to 1.\n",
    "\n",
    "backward():\n",
    "The derivative of softmax is more complex ‚Äî it requires a Jacobian matrix.\n",
    "\n",
    "For each sample, we compute:\n",
    "\n",
    "ùêΩ=\n",
    "diag\n",
    "(\n",
    "ùë†\n",
    ")\n",
    "‚àí\n",
    "ùë†\n",
    "‚ãÖ\n",
    "ùë†\n",
    "ùëá\n",
    "J=diag(s)‚àís‚ãÖs \n",
    "T\n",
    " \n",
    "where \n",
    "ùë†\n",
    "s is the softmax output vector.\n",
    "\n",
    "This matrix is then multiplied by the incoming gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be236fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d241a",
   "metadata": {},
   "source": [
    "###  Activation Function: Sigmoid\n",
    "\n",
    "The **sigmoid** activation function is used in the **output layer** for **binary classification** problems.  \n",
    "It transforms input values into probabilities between 0 and 1.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Forward Pass:\n",
    "Applies the sigmoid function element-wise to each input:\n",
    "- For large positive input: output approaches 1\n",
    "- For large negative input: output approaches 0\n",
    "\n",
    "The output is stored for use during backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "###  Backward Pass:\n",
    "To propagate gradients backward, we use the derivative of sigmoid:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "The gradient of the loss with respect to the inputs is:\n",
    "\n",
    "$$\n",
    "\\text{dinputs} = \\text{dvalues} \\cdot \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "This controls how much each input contributed to the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1af0ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9340a",
   "metadata": {},
   "source": [
    "###  Optimizer: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is the most basic optimizer used in training neural networks.  \n",
    "It updates weights and biases by moving in the direction **opposite** to the gradient of the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "###  Update Rule (Vanilla SGD):\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta L\n",
    "$$\n",
    "Where:\n",
    "- \\( \\theta \\): parameters (weights/biases)\n",
    "- \\( \\alpha \\): learning rate\n",
    "- \\( \\nabla_\\theta L \\): gradient of the loss with respect to \\( \\theta \\)\n",
    "\n",
    "---\n",
    "\n",
    "###  Optional Extensions:\n",
    "- **Learning rate decay**: Reduces learning rate over time:\n",
    "$$\n",
    "\\alpha_t = \\frac{\\alpha_0}{1 + \\text{decay} \\cdot t}\n",
    "$$\n",
    "\n",
    "- **Momentum**: Accelerates convergence by using an exponentially decaying moving average of past gradients:\n",
    "$$\n",
    "v_t = \\beta v_{t-1} - \\alpha \\nabla_\\theta L \\\\\n",
    "\\theta = \\theta + v_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Method Summary:\n",
    "- `pre_update_params()`: Adjusts learning rate using decay.\n",
    "- `update_params(layer)`: Applies gradient updates with/without momentum.\n",
    "- `post_update_params()`: Increments the iteration count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4d6c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5d863",
   "metadata": {},
   "source": [
    "###  Optimizer: Adagrad\n",
    "\n",
    "**Adagrad** is an adaptive learning rate optimizer.  \n",
    "It adapts the learning rate for each parameter individually based on how frequently it's updated.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Concept:\n",
    "- Parameters with **frequent updates** receive **smaller learning rates**.\n",
    "- Parameters with **infrequent updates** get **larger learning rates**.\n",
    "- It uses an accumulator (cache) of squared gradients.\n",
    "\n",
    "---\n",
    "\n",
    "###  Update Rule:\n",
    "For each parameter \\( \\theta \\):\n",
    "$$\n",
    "G_t = G_{t-1} + \\nabla_\\theta^2 L \\\\\n",
    "\\theta = \\theta - \\frac{\\alpha}{\\sqrt{G_t} + \\epsilon} \\cdot \\nabla_\\theta L\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( G_t \\): sum of squared gradients\n",
    "- \\( \\epsilon \\): small number to avoid division by zero\n",
    "\n",
    "---\n",
    "\n",
    "###  Method Summary:\n",
    "- `pre_update_params()`: Applies learning rate decay.\n",
    "- `update_params(layer)`: Uses cached squared gradients to scale the learning rate.\n",
    "- `post_update_params()`: Increments iteration count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e716f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fbaea",
   "metadata": {},
   "source": [
    "### Optimizer: RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "**RMSprop** is an adaptive learning rate optimizer, like Adagrad, but it uses an **exponentially decaying average** of past squared gradients instead of summing them.  \n",
    "This prevents the learning rate from shrinking too much over time.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Concept:\n",
    "Instead of accumulating all squared gradients, RMSprop maintains a **moving average**:\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\rho \\cdot E[g^2]_{t-1} + (1 - \\rho) \\cdot g_t^2 \\\\\n",
    "\\theta = \\theta - \\frac{\\alpha}{\\sqrt{E[g^2]_t} + \\epsilon} \\cdot g_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\rho \\): decay rate (typically 0.9)\n",
    "- \\( \\epsilon \\): small constant to prevent division by zero\n",
    "\n",
    "---\n",
    "\n",
    "###  Method Summary:\n",
    "- `pre_update_params()`: Handles learning rate decay.\n",
    "- `update_params(layer)`: Updates weights using RMS of recent gradients.\n",
    "- `post_update_params()`: Increments the iteration counter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01bd8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Parameter update with normalization\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b12c7",
   "metadata": {},
   "source": [
    "###  Optimizer: Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Adam** combines the advantages of:\n",
    "- **Momentum** (uses moving averages of gradients)\n",
    "- **RMSprop** (uses moving averages of squared gradients)\n",
    "\n",
    "Adam is widely used for training deep neural networks because it works well **out of the box** with little tuning.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Concepts:\n",
    "\n",
    "- **Momentum estimate** (first moment):\n",
    "$$\n",
    "m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n",
    "$$\n",
    "\n",
    "- **RMS estimate** (second moment):\n",
    "$$\n",
    "v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2\n",
    "$$\n",
    "\n",
    "- **Bias correction**:\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "- **Parameter update**:\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Method Summary:\n",
    "- `pre_update_params()`: Adjusts learning rate with decay.\n",
    "- `update_params(layer)`: Applies moment + RMS scaling + correction.\n",
    "- `post_update_params()`: Increments iteration count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01d23ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "            (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "            (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f35e38",
   "metadata": {},
   "source": [
    "###  Base Class: Loss\n",
    "\n",
    "The `Loss` class serves as a **base** for all loss functions. It provides:\n",
    "\n",
    "- A method to compute **regularization loss** using L1 and L2 penalties.\n",
    "- A `calculate()` method that:\n",
    "  1. Calls the child class's `forward()` to compute sample-wise loss.\n",
    "  2. Averages it to compute the batch loss.\n",
    "\n",
    "---\n",
    "\n",
    "###  Regularization Loss Formula:\n",
    "\n",
    "- L1 Regularization:\n",
    "$$\n",
    "L_{1} = \\lambda \\sum |w|\n",
    "$$\n",
    "\n",
    "- L2 Regularization:\n",
    "$$\n",
    "L_{2} = \\lambda \\sum w^2\n",
    "$$\n",
    "\n",
    "These are added to the main data loss to compute total loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0879773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c05627",
   "metadata": {},
   "source": [
    "###  Loss: Categorical Cross-Entropy\n",
    "\n",
    "**Categorical Cross-Entropy** is used when the model outputs a **probability distribution** over multiple classes.\n",
    "\n",
    "---\n",
    "\n",
    "###  Forward Pass:\n",
    "\n",
    "Given predicted probabilities \\( \\hat{y} \\) and true labels \\( y \\), the loss for a sample is:\n",
    "\n",
    "$$\n",
    "L = -\\log(\\hat{y}_{\\text{true class}})\n",
    "$$\n",
    "\n",
    "- If labels are **sparse**: use `y_true` as indices.\n",
    "- If labels are **one-hot encoded**: use element-wise multiplication and sum.\n",
    "\n",
    "We clip \\( \\hat{y} \\) to avoid log(0) and numerical instability.\n",
    "\n",
    "---\n",
    "\n",
    "###  Backward Pass:\n",
    "\n",
    "The gradient of the loss w.r.t. the predictions is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y_{\\text{true}}}{\\hat{y}}\n",
    "$$\n",
    "\n",
    "Gradients are normalized by dividing by number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c86d662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4886768d",
   "metadata": {},
   "source": [
    "###  Combined Softmax Activation and Categorical Cross-Entropy Loss\n",
    "\n",
    "This class combines two operations:\n",
    "1. **Softmax activation** (turns logits into probabilities)\n",
    "2. **Categorical cross-entropy loss** (computes log loss from probabilities)\n",
    "\n",
    "---\n",
    "\n",
    "###  Why Combine?\n",
    "\n",
    "When computed separately, softmax and cross-entropy can become **numerically unstable**.  \n",
    "This class computes their derivative directly, **bypassing the need to compute a full Jacobian matrix**, and simplifies the backward pass to:\n",
    "\n",
    "$$\n",
    "\\text{dinputs} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\hat{y} \\): predicted probabilities\n",
    "- \\( y \\): one-hot encoded or sparse true labels\n",
    "\n",
    "---\n",
    "\n",
    "###  Method Summary:\n",
    "- `forward(inputs, y_true)`: Applies softmax + computes cross-entropy.\n",
    "- `backward(dvalues, y_true)`: Computes simplified gradient in one step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84f7775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e5e99",
   "metadata": {},
   "source": [
    "###  Loss: Binary Cross-Entropy\n",
    "\n",
    "This loss is used when the model performs **binary classification**, i.e., predicting a value between 0 and 1 using the **sigmoid activation** in the output layer.\n",
    "\n",
    "---\n",
    "\n",
    "###  Forward Pass:\n",
    "\n",
    "The binary cross-entropy loss for predicted probability \\( \\hat{y} \\) and true label \\( y \\in \\{0, 1\\} \\) is:\n",
    "\n",
    "$$\n",
    "L = -\\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "- The output \\( \\hat{y} \\) is **clipped** between \\( 1 \\times 10^{-7} \\) and \\( 1 - 10^{-7} \\) to avoid log(0).\n",
    "- The average over all output neurons (axis = -1) is computed.\n",
    "\n",
    "---\n",
    "\n",
    "###  Backward Pass:\n",
    "\n",
    "The gradient of binary cross-entropy w.r.t. predictions:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = -\\left[ \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right]\n",
    "$$\n",
    "\n",
    "Then normalize by dividing over:\n",
    "- number of **outputs** per sample\n",
    "- number of **samples**\n",
    "\n",
    "This provides the gradient to pass back to the previous layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9149af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1292b2e",
   "metadata": {},
   "source": [
    "###  Full Neural Network Training (Binary Classification)\n",
    "\n",
    "Now we use everything we've built:\n",
    "\n",
    "- A two-layer neural network:\n",
    "  - Dense ‚Üí ReLU ‚Üí Dense ‚Üí Sigmoid\n",
    "- Binary cross-entropy loss\n",
    "- Adam optimizer with learning rate decay\n",
    "- Regularization to reduce overfitting\n",
    "\n",
    "We use a **spiral dataset** with 2 classes and 2D inputs.\n",
    "\n",
    "---\n",
    "\n",
    "###  Network Summary:\n",
    "\n",
    "- **Layer 1**:\n",
    "  - Input: 2 features\n",
    "  - Output: 64 neurons\n",
    "  - Activation: ReLU\n",
    "  - L2 regularization\n",
    "\n",
    "- **Layer 2**:\n",
    "  - Input: 64 features\n",
    "  - Output: 1 neuron\n",
    "  - Activation: Sigmoid\n",
    "\n",
    "- **Loss**: Binary cross-entropy\n",
    "\n",
    "- **Optimizer**: Adam (adaptive learning + momentum)\n",
    "\n",
    "---\n",
    "\n",
    "###  Training Loop:\n",
    "\n",
    "1. Forward pass ‚Üí through all layers + activations\n",
    "2. Compute loss:\n",
    "   - Data loss + L2 regularization penalty\n",
    "3. Compute accuracy:\n",
    "   - Use threshold: \\( \\hat{y} > 0.5 \\rightarrow 1 \\)\n",
    "4. Backward pass ‚Üí gradients for all layers\n",
    "5. Optimizer updates weights\n",
    "6. Repeat for many epochs (10,000+)\n",
    "\n",
    "---\n",
    "\n",
    "###  Evaluation:\n",
    "At the end, we evaluate the model using **unseen test data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0650a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "accuracy_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "810b7657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, loss: 0.693 (data: 0.693, reg: 0.000), lr: 0.001\n",
      "epoch: 100, acc: 0.630, loss: 0.674 (data: 0.673, reg: 0.001), lr: 0.0009999505024501287\n",
      "epoch: 200, acc: 0.625, loss: 0.669 (data: 0.668, reg: 0.001), lr: 0.0009999005098992651\n",
      "epoch: 300, acc: 0.645, loss: 0.665 (data: 0.663, reg: 0.002), lr: 0.000999850522346909\n",
      "epoch: 400, acc: 0.650, loss: 0.659 (data: 0.657, reg: 0.002), lr: 0.0009998005397923115\n",
      "epoch: 500, acc: 0.675, loss: 0.648 (data: 0.644, reg: 0.004), lr: 0.0009997505622347225\n",
      "epoch: 600, acc: 0.720, loss: 0.632 (data: 0.626, reg: 0.006), lr: 0.0009997005896733929\n",
      "epoch: 700, acc: 0.770, loss: 0.614 (data: 0.604, reg: 0.010), lr: 0.0009996506221075735\n",
      "epoch: 800, acc: 0.775, loss: 0.594 (data: 0.579, reg: 0.015), lr: 0.000999600659536515\n",
      "epoch: 900, acc: 0.775, loss: 0.575 (data: 0.556, reg: 0.020), lr: 0.0009995507019594694\n",
      "epoch: 1000, acc: 0.785, loss: 0.560 (data: 0.536, reg: 0.024), lr: 0.000999500749375687\n",
      "epoch: 1100, acc: 0.790, loss: 0.547 (data: 0.520, reg: 0.028), lr: 0.0009994508017844195\n",
      "epoch: 1200, acc: 0.790, loss: 0.536 (data: 0.505, reg: 0.031), lr: 0.0009994008591849186\n",
      "epoch: 1300, acc: 0.795, loss: 0.524 (data: 0.491, reg: 0.033), lr: 0.0009993509215764362\n",
      "epoch: 1400, acc: 0.795, loss: 0.513 (data: 0.478, reg: 0.036), lr: 0.0009993009889582235\n",
      "epoch: 1500, acc: 0.810, loss: 0.502 (data: 0.464, reg: 0.038), lr: 0.0009992510613295335\n",
      "epoch: 1600, acc: 0.810, loss: 0.493 (data: 0.452, reg: 0.040), lr: 0.0009992011386896176\n",
      "epoch: 1700, acc: 0.820, loss: 0.484 (data: 0.441, reg: 0.043), lr: 0.0009991512210377285\n",
      "epoch: 1800, acc: 0.825, loss: 0.475 (data: 0.431, reg: 0.044), lr: 0.0009991013083731183\n",
      "epoch: 1900, acc: 0.825, loss: 0.468 (data: 0.422, reg: 0.046), lr: 0.0009990514006950402\n",
      "epoch: 2000, acc: 0.825, loss: 0.461 (data: 0.413, reg: 0.048), lr: 0.0009990014980027463\n",
      "epoch: 2100, acc: 0.835, loss: 0.454 (data: 0.405, reg: 0.049), lr: 0.0009989516002954898\n",
      "epoch: 2200, acc: 0.840, loss: 0.448 (data: 0.398, reg: 0.050), lr: 0.000998901707572524\n",
      "epoch: 2300, acc: 0.845, loss: 0.443 (data: 0.392, reg: 0.051), lr: 0.0009988518198331018\n",
      "epoch: 2400, acc: 0.850, loss: 0.438 (data: 0.386, reg: 0.052), lr: 0.0009988019370764769\n",
      "epoch: 2500, acc: 0.855, loss: 0.433 (data: 0.380, reg: 0.053), lr: 0.0009987520593019025\n",
      "epoch: 2600, acc: 0.865, loss: 0.428 (data: 0.375, reg: 0.053), lr: 0.000998702186508632\n",
      "epoch: 2700, acc: 0.865, loss: 0.424 (data: 0.370, reg: 0.054), lr: 0.00099865231869592\n",
      "epoch: 2800, acc: 0.865, loss: 0.420 (data: 0.365, reg: 0.055), lr: 0.0009986024558630198\n",
      "epoch: 2900, acc: 0.870, loss: 0.416 (data: 0.361, reg: 0.055), lr: 0.0009985525980091856\n",
      "epoch: 3000, acc: 0.870, loss: 0.412 (data: 0.357, reg: 0.055), lr: 0.000998502745133672\n",
      "epoch: 3100, acc: 0.875, loss: 0.409 (data: 0.353, reg: 0.056), lr: 0.0009984528972357331\n",
      "epoch: 3200, acc: 0.875, loss: 0.405 (data: 0.349, reg: 0.056), lr: 0.0009984030543146237\n",
      "epoch: 3300, acc: 0.880, loss: 0.402 (data: 0.346, reg: 0.056), lr: 0.0009983532163695982\n",
      "epoch: 3400, acc: 0.885, loss: 0.399 (data: 0.343, reg: 0.057), lr: 0.000998303383399912\n",
      "epoch: 3500, acc: 0.890, loss: 0.396 (data: 0.340, reg: 0.057), lr: 0.0009982535554048193\n",
      "epoch: 3600, acc: 0.890, loss: 0.394 (data: 0.337, reg: 0.057), lr: 0.000998203732383576\n",
      "epoch: 3700, acc: 0.890, loss: 0.391 (data: 0.334, reg: 0.057), lr: 0.0009981539143354365\n",
      "epoch: 3800, acc: 0.890, loss: 0.388 (data: 0.331, reg: 0.057), lr: 0.0009981041012596574\n",
      "epoch: 3900, acc: 0.890, loss: 0.385 (data: 0.328, reg: 0.057), lr: 0.0009980542931554933\n",
      "epoch: 4000, acc: 0.900, loss: 0.378 (data: 0.321, reg: 0.057), lr: 0.0009980044900222008\n",
      "epoch: 4100, acc: 0.900, loss: 0.373 (data: 0.316, reg: 0.057), lr: 0.0009979546918590348\n",
      "epoch: 4200, acc: 0.900, loss: 0.370 (data: 0.313, reg: 0.057), lr: 0.0009979048986652524\n",
      "epoch: 4300, acc: 0.900, loss: 0.368 (data: 0.310, reg: 0.058), lr: 0.000997855110440109\n",
      "epoch: 4400, acc: 0.900, loss: 0.365 (data: 0.307, reg: 0.058), lr: 0.0009978053271828614\n",
      "epoch: 4500, acc: 0.905, loss: 0.363 (data: 0.305, reg: 0.058), lr: 0.0009977555488927658\n",
      "epoch: 4600, acc: 0.905, loss: 0.361 (data: 0.303, reg: 0.058), lr: 0.000997705775569079\n",
      "epoch: 4700, acc: 0.905, loss: 0.358 (data: 0.300, reg: 0.058), lr: 0.0009976560072110577\n",
      "epoch: 4800, acc: 0.910, loss: 0.354 (data: 0.296, reg: 0.058), lr: 0.0009976062438179587\n",
      "epoch: 4900, acc: 0.910, loss: 0.351 (data: 0.292, reg: 0.059), lr: 0.0009975564853890394\n",
      "epoch: 5000, acc: 0.910, loss: 0.348 (data: 0.288, reg: 0.060), lr: 0.000997506731923557\n",
      "epoch: 5100, acc: 0.910, loss: 0.345 (data: 0.285, reg: 0.060), lr: 0.0009974569834207687\n",
      "epoch: 5200, acc: 0.915, loss: 0.342 (data: 0.282, reg: 0.061), lr: 0.0009974072398799322\n",
      "epoch: 5300, acc: 0.915, loss: 0.340 (data: 0.278, reg: 0.061), lr: 0.0009973575013003048\n",
      "epoch: 5400, acc: 0.915, loss: 0.337 (data: 0.276, reg: 0.062), lr: 0.0009973077676811448\n",
      "epoch: 5500, acc: 0.915, loss: 0.335 (data: 0.273, reg: 0.062), lr: 0.00099725803902171\n",
      "epoch: 5600, acc: 0.915, loss: 0.333 (data: 0.271, reg: 0.062), lr: 0.0009972083153212581\n",
      "epoch: 5700, acc: 0.915, loss: 0.331 (data: 0.269, reg: 0.062), lr: 0.000997158596579048\n",
      "epoch: 5800, acc: 0.915, loss: 0.329 (data: 0.267, reg: 0.062), lr: 0.0009971088827943377\n",
      "epoch: 5900, acc: 0.915, loss: 0.328 (data: 0.265, reg: 0.062), lr: 0.0009970591739663862\n",
      "epoch: 6000, acc: 0.915, loss: 0.326 (data: 0.264, reg: 0.062), lr: 0.0009970094700944517\n",
      "epoch: 6100, acc: 0.915, loss: 0.324 (data: 0.262, reg: 0.062), lr: 0.0009969597711777935\n",
      "epoch: 6200, acc: 0.915, loss: 0.322 (data: 0.261, reg: 0.062), lr: 0.00099691007721567\n",
      "epoch: 6300, acc: 0.920, loss: 0.318 (data: 0.256, reg: 0.062), lr: 0.000996860388207341\n",
      "epoch: 6400, acc: 0.925, loss: 0.315 (data: 0.253, reg: 0.062), lr: 0.0009968107041520655\n",
      "epoch: 6500, acc: 0.925, loss: 0.310 (data: 0.249, reg: 0.062), lr: 0.000996761025049103\n",
      "epoch: 6600, acc: 0.935, loss: 0.307 (data: 0.245, reg: 0.062), lr: 0.000996711350897713\n",
      "epoch: 6700, acc: 0.935, loss: 0.304 (data: 0.243, reg: 0.062), lr: 0.0009966616816971556\n",
      "epoch: 6800, acc: 0.935, loss: 0.303 (data: 0.241, reg: 0.062), lr: 0.00099661201744669\n",
      "epoch: 6900, acc: 0.935, loss: 0.301 (data: 0.239, reg: 0.062), lr: 0.0009965623581455767\n",
      "epoch: 7000, acc: 0.940, loss: 0.299 (data: 0.238, reg: 0.062), lr: 0.000996512703793076\n",
      "epoch: 7100, acc: 0.935, loss: 0.298 (data: 0.236, reg: 0.062), lr: 0.0009964630543884481\n",
      "epoch: 7200, acc: 0.940, loss: 0.296 (data: 0.234, reg: 0.061), lr: 0.0009964134099309536\n",
      "epoch: 7300, acc: 0.935, loss: 0.294 (data: 0.233, reg: 0.061), lr: 0.0009963637704198528\n",
      "epoch: 7400, acc: 0.940, loss: 0.293 (data: 0.231, reg: 0.061), lr: 0.0009963141358544066\n",
      "epoch: 7500, acc: 0.940, loss: 0.291 (data: 0.230, reg: 0.061), lr: 0.000996264506233876\n",
      "epoch: 7600, acc: 0.940, loss: 0.290 (data: 0.229, reg: 0.061), lr: 0.0009962148815575223\n",
      "epoch: 7700, acc: 0.940, loss: 0.289 (data: 0.227, reg: 0.061), lr: 0.000996165261824606\n",
      "epoch: 7800, acc: 0.940, loss: 0.287 (data: 0.226, reg: 0.061), lr: 0.0009961156470343895\n",
      "epoch: 7900, acc: 0.940, loss: 0.286 (data: 0.225, reg: 0.061), lr: 0.0009960660371861334\n",
      "epoch: 8000, acc: 0.940, loss: 0.285 (data: 0.224, reg: 0.061), lr: 0.0009960164322790998\n",
      "epoch: 8100, acc: 0.940, loss: 0.282 (data: 0.222, reg: 0.060), lr: 0.0009959668323125503\n",
      "epoch: 8200, acc: 0.940, loss: 0.280 (data: 0.220, reg: 0.060), lr: 0.000995917237285747\n",
      "epoch: 8300, acc: 0.940, loss: 0.279 (data: 0.219, reg: 0.060), lr: 0.000995867647197952\n",
      "epoch: 8400, acc: 0.940, loss: 0.277 (data: 0.218, reg: 0.060), lr: 0.0009958180620484277\n",
      "epoch: 8500, acc: 0.940, loss: 0.276 (data: 0.216, reg: 0.060), lr: 0.0009957684818364362\n",
      "epoch: 8600, acc: 0.940, loss: 0.275 (data: 0.215, reg: 0.059), lr: 0.0009957189065612402\n",
      "epoch: 8700, acc: 0.945, loss: 0.273 (data: 0.214, reg: 0.059), lr: 0.000995669336222102\n",
      "epoch: 8800, acc: 0.945, loss: 0.272 (data: 0.213, reg: 0.059), lr: 0.000995619770818285\n",
      "epoch: 8900, acc: 0.945, loss: 0.271 (data: 0.212, reg: 0.059), lr: 0.0009955702103490519\n",
      "epoch: 9000, acc: 0.945, loss: 0.270 (data: 0.211, reg: 0.059), lr: 0.000995520654813666\n",
      "epoch: 9100, acc: 0.945, loss: 0.269 (data: 0.210, reg: 0.059), lr: 0.0009954711042113903\n",
      "epoch: 9200, acc: 0.945, loss: 0.268 (data: 0.210, reg: 0.058), lr: 0.0009954215585414883\n",
      "epoch: 9300, acc: 0.945, loss: 0.267 (data: 0.209, reg: 0.058), lr: 0.000995372017803224\n",
      "epoch: 9400, acc: 0.945, loss: 0.266 (data: 0.208, reg: 0.058), lr: 0.0009953224819958604\n",
      "epoch: 9500, acc: 0.945, loss: 0.265 (data: 0.207, reg: 0.058), lr: 0.000995272951118662\n",
      "epoch: 9600, acc: 0.945, loss: 0.264 (data: 0.206, reg: 0.057), lr: 0.0009952234251708924\n",
      "epoch: 9700, acc: 0.945, loss: 0.263 (data: 0.206, reg: 0.057), lr: 0.000995173904151816\n",
      "epoch: 9800, acc: 0.945, loss: 0.262 (data: 0.205, reg: 0.057), lr: 0.0009951243880606966\n",
      "epoch: 9900, acc: 0.945, loss: 0.261 (data: 0.204, reg: 0.057), lr: 0.0009950748768967994\n",
      "epoch: 10000, acc: 0.945, loss: 0.260 (data: 0.204, reg: 0.056), lr: 0.0009950253706593885\n",
      "validation, acc: 0.920, loss: 0.237\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "y = y.reshape(-1, 1)  # Reshape to column vector\n",
    "\n",
    "# Create layers\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# Create loss and optimizer\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Compute loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    reg_loss = loss_function.regularization_loss(dense1) + \\\n",
    "               loss_function.regularization_loss(dense2)\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # Log progress\n",
    "    loss_history.append(loss)\n",
    "    accuracy_history.append(accuracy)\n",
    "\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f} '\n",
    "              f'(data: {data_loss:.3f}, reg: {reg_loss:.3f}), '\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validation\n",
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63a36de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, label='Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_history, label='Accuracy', color='green')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
