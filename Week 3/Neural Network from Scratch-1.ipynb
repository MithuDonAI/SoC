{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d22883",
   "metadata": {},
   "source": [
    "### 1. What Is a Neural Network?\n",
    "\n",
    "A **neural network** is a computational model inspired by the structure of the brain. It is designed to approximate complex functions that map input data to output labels or values.\n",
    "\n",
    "At its core, it performs a series of matrix operations that transform inputs through layers of neurons using trainable parameters called **weights** and **biases**.\n",
    "\n",
    "The **goal of training** a neural network is to find the optimal set of weights and biases that minimize the difference between predicted and actual outputs — a quantity measured by the **loss function**.\n",
    "\n",
    "A basic feedforward neural network can be represented as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(X) = A_L = \\phi_L(W_L \\cdot A_{L-1} + b_L)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$: input data\n",
    "- $W_i$, $b_i$: weights and biases of layer $i$\n",
    "- $\\phi_i$: activation function at layer $i$\n",
    "- $A_L$: final output of the network\n",
    "- $\\hat{y}$: predicted output\n",
    "- $y$: true label\n",
    "\n",
    "The training involves two key phases:\n",
    "1. **Forward pass**: compute output predictions $\\hat{y}$.\n",
    "2. **Backward pass**: compute gradients of the loss $\\mathcal{L}$ and update weights to reduce this loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3168df",
   "metadata": {},
   "source": [
    "### 2. Dataset — Spiral Classification\n",
    "\n",
    "We use a synthetic dataset called the **spiral dataset**, where each class lies along a spiral-shaped trajectory in 2D space.\n",
    "\n",
    "This task is **not linearly separable**, so it is a good test for neural networks that can learn non-linear boundaries.\n",
    "\n",
    "Each data point:\n",
    "- $X_i \\in \\mathbb{R}^2$: a 2D feature vector\n",
    "- $y_i \\in \\{0, 1\\}$: its binary class label\n",
    "\n",
    "We generate 100 samples per class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "y = y.reshape(-1, 1)  # reshape to (100, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9f9d9",
   "metadata": {},
   "source": [
    "### 3. What Is a Dense Layer? (Weights and Biases)\n",
    "\n",
    "A **layer** is a block of a neural network that transforms input data using learnable parameters. The most common is the **dense layer** (fully connected layer).\n",
    "\n",
    "Each neuron in a layer performs:\n",
    "\n",
    "$$\n",
    "z_i = \\sum_{j=1}^n x_j w_{ji} + b_i = \\vec{x} \\cdot \\vec{w}_i + b_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_j$: input features\n",
    "- $w_{ji}$: weight for feature $j$ for neuron $i$\n",
    "- $b_i$: bias for neuron $i$\n",
    "- $z_i$: output (logit) for neuron $i$\n",
    "\n",
    "For a batch of inputs $X \\in \\mathbb{R}^{m \\times n}$:\n",
    "\\[\n",
    "Z = XW + b\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{n \\times h}$: weights matrix\n",
    "- $b \\in \\mathbb{R}^{1 \\times h}$: biases vector\n",
    "- $Z \\in \\mathbb{R}^{m \\times h}$: output of layer\n",
    "\n",
    "**Why weights and biases?**\n",
    "- **Weights ($W$)** scale input features; they define what the neuron is sensitive to.\n",
    "- **Biases ($b$)** allow the activation to shift left/right, enabling better fitting of data.\n",
    "\n",
    "These parameters are **learned** during training to reduce the prediction error (loss).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa101ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2be0c",
   "metadata": {},
   "source": [
    "### 4. Forward Pass — Making Predictions\n",
    "\n",
    "The **forward pass** refers to computing the output of the neural network from input to output layer.\n",
    "\n",
    "Each layer computes:\n",
    "\n",
    "$$\n",
    "Z = XW + b, \\quad A = \\phi(Z)\n",
    "$$\n",
    "\n",
    "Where $\\phi$ is the activation function (e.g. ReLU, Sigmoid, etc.).\n",
    "\n",
    "The result of the forward pass is a prediction $\\hat{y}$. We compare this prediction with the ground truth $y$ using a **loss function**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bcdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "dense1.forward(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8d64e",
   "metadata": {},
   "source": [
    "### 5. What Is a Gradient? (Backpropagation)\n",
    "\n",
    "A **gradient** is a partial derivative of a function with respect to its input — it tells us how sensitive the output is to changes in that input.\n",
    "\n",
    "During training, we want to minimize the **loss function** $\\mathcal{L}(\\hat{y}, y)$.\n",
    "\n",
    "To do this, we compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "These gradients tell us how to **tweak weights and biases** to reduce the error.\n",
    "\n",
    "This process is called **backpropagation**.\n",
    "\n",
    "For a dense layer, the gradients are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = X^\\top \\delta, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum \\delta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Regularization terms\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd0a8b",
   "metadata": {},
   "source": [
    "### 6. ReLU Activation Function\n",
    "\n",
    "ReLU (Rectified Linear Unit) introduces **non-linearity** into the network:\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Why ReLU?\n",
    "- Keeps positive values\n",
    "- Sets negatives to zero\n",
    "- Fast and efficient\n",
    "- Helps avoid vanishing gradient issues\n",
    "\n",
    "Its derivative is simple:\n",
    "\n",
    "$$\n",
    "f'(x) =\n",
    "\\begin{cases}\n",
    "1 & x > 0 \\\\\n",
    "0 & x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is used in the backward pass to propagate gradients only through active neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
