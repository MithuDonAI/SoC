{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e023a955",
   "metadata": {},
   "source": [
    "### 7. Second Dense Layer — Output Neuron\n",
    "\n",
    "Our second layer takes the 64 outputs from the previous (hidden) layer and maps them to a **single output neuron**:\n",
    "\n",
    "$$\n",
    "z^{(2)} = \\vec{a}^{(1)} \\cdot \\vec{w}^{(2)} + b^{(2)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\vec{a}^{(1)}$: activations from first layer (after ReLU)\n",
    "- $\\vec{w}^{(2)}$: weights connecting 64 hidden units to this output neuron\n",
    "- $b^{(2)}$: scalar bias\n",
    "\n",
    "This output neuron gives a **logit** — a raw prediction score — that will be passed through a **Sigmoid activation** to convert it to probability.\n",
    "\n",
    "Since we are doing **binary classification**, the output should lie in the range [0, 1], indicating probability of belonging to class 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 = Layer_Dense(64, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a02475",
   "metadata": {},
   "source": [
    "### 8. Sigmoid Activation Function\n",
    "\n",
    "Sigmoid is an S-shaped curve that maps any real number to a value in (0, 1):\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z$: input logit (raw score)\n",
    "- $\\sigma(z)$: output probability\n",
    "\n",
    "This makes it perfect for binary classification — the output becomes interpretable as the **probability that a sample belongs to class 1**.\n",
    "\n",
    "### Derivative (used in backward pass):\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "This derivative shows how the output changes with respect to the input — critical for computing gradients during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation2 = Activation_Sigmoid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db451b",
   "metadata": {},
   "source": [
    "### 9. Final Output and Classification\n",
    "\n",
    "The output of the Sigmoid layer is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z^{(2)}) \\in (0, 1)\n",
    "$$\n",
    "\n",
    "We interpret this as:\n",
    "\n",
    "- $\\hat{y} > 0.5 \\Rightarrow$ predict class 1\n",
    "- $\\hat{y} \\leq 0.5 \\Rightarrow$ predict class 0\n",
    "\n",
    "This simple rule turns a probability into a **class prediction**.\n",
    "\n",
    "For 2D visualization tasks (like spiral data), this threshold effectively draws a decision boundary in space between the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (activation2.output > 0.5) * 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7934c09",
   "metadata": {},
   "source": [
    "### 10. What Is a Loss Function?\n",
    "\n",
    "A **loss function** quantifies how wrong the network's predictions are.\n",
    "\n",
    "If $\\hat{y}$ is the prediction and $y$ is the true label, then the loss is a **measure of error**.\n",
    "\n",
    "In **binary classification**, we use:\n",
    "\n",
    "### Binary Cross-Entropy Loss:\n",
    "\n",
    "For a single sample:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = -\\left[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})\\right]\n",
    "$$\n",
    "\n",
    "Why this form?\n",
    "- It heavily penalizes confident but wrong predictions (e.g., $\\hat{y} = 0.99$ but $y = 0$)\n",
    "- It is derived from maximum likelihood estimation under a Bernoulli model\n",
    "\n",
    "We compute the average loss over the dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{avg} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(\\hat{y}_i, y_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        return np.mean(sample_losses, axis=-1)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = Loss_BinaryCrossentropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc7a43",
   "metadata": {},
   "source": [
    "### 11. Why Do We Train?\n",
    "\n",
    "Our aim is to **minimize the loss function** by adjusting the weights and biases:\n",
    "\n",
    "This means:\n",
    "- If the loss is high, the model is making poor predictions.\n",
    "- If the loss is low, the model is making accurate predictions.\n",
    "\n",
    "### How to Minimize?\n",
    "\n",
    "We use **Gradient Descent** — a method that tweaks each parameter in the direction that reduces the loss.\n",
    "\n",
    "Each parameter update is:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$: a parameter (weight or bias)\n",
    "- $\\eta$: learning rate\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$: gradient of loss\n",
    "\n",
    "This process repeats across many **epochs**, allowing the network to learn the best parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d513928",
   "metadata": {},
   "source": [
    "### 12. Accuracy\n",
    "\n",
    "Accuracy is the **fraction of correctly predicted labels**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "In binary classification, if:\n",
    "\n",
    "- $\\hat{y} > 0.5$ and $y = 1$ → correct\n",
    "- $\\hat{y} \\le 0.5$ and $y = 0$ → correct\n",
    "\n",
    "All other cases are incorrect. We calculate this metric after every epoch to monitor performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(predictions == y)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
